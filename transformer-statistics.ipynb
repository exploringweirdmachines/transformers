{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdaa735b490>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "context_length = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "number_embeddings = 384\n",
    "n_heads = 6    # 384/6 - every head is 64 dim\n",
    "n_layer = 6\n",
    "\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('shake.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(number_embeddings, head_size, bias=False)\n",
    "        self.query = nn.Linear(number_embeddings, head_size, bias=False)\n",
    "        self.value = nn.Linear(number_embeddings, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length))) # this is called a buffer, it's not a parameter, and if you have to call it, you have to assign it to the module using a register buffer\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # we use scaled attention here\n",
    "        weights = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) \n",
    "\n",
    "        # ensure it doesn't communicate with the past - decoder attention\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "\n",
    "        weights = self.dropout(weights)   # randomly prevent some of the nodes from communicating\n",
    "        \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "\n",
    "        # the output\n",
    "        out = weights @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(number_embeddings, number_embeddings)  # we add this projection, this is just a linear transformation \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat on the C dim\n",
    "        out = self.dropout(self.proj(out)) # projection, this is just a linear transformation on the outcome of the concatenation layer from above this line\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a single linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, num_embd):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_embd, 4 * num_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * num_embd, num_embd), # this is the projection layer\n",
    "            nn.Dropout(dropout)  # Dropout is something that you can add right before the residual connection back or right before the connection back into the original pathway\n",
    "        )\n",
    "        # we can have the projection layer here but it was placed above this line in the sequential container\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we added some changes for layernorm\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, num_embd, num_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = num_embd // num_head # this will become size 8\n",
    "\n",
    "        # Communication\n",
    "        self.sa = MultiHeadAttention(num_head, head_size) \n",
    "\n",
    "        # Computation\n",
    "        self.ffwd = FeedFoward(num_embd)\n",
    "\n",
    "        # layernorm\n",
    "        self.ln1 = nn.LayerNorm(num_embd)\n",
    "        self.ln2 = nn.LayerNorm(num_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x + self.sa(self.ln1(x))     # we apply layer norm before going into the self attention\n",
    "        x = x + self.ffwd(self.ln2(x))       # we apply layer norm before going into the feed forward\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation to plug the self attention into our model\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, number_embeddings)\n",
    "        self.position_embedding_table = nn.Embedding(context_length, number_embeddings)\n",
    "        self.blocks = nn.Sequential(*[Block(number_embeddings, num_head=4) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(number_embeddings)\n",
    "        self.lm_head = nn.Linear(number_embeddings, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B,T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=device))  # (T, C) from 0 to t-1\n",
    "\n",
    "        x = token_embeddings + position_embeddings # (B,T,C)\n",
    "\n",
    "        # apply heads of attention\n",
    "        x = self.blocks(x)  \n",
    "\n",
    "        # we went way too fast on calculating the logits before, so now we put before this the feed forward\n",
    "        logits = self.lm_head(x) # (B,T, vocab_size)\n",
    "\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the lastcontext_length tokens\n",
    "            idx_cond = idx[:, -context_length:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.9664, val loss 4.0026\n",
      "step 10: train loss 2.9530, val loss 2.9735\n",
      "step 20: train loss 2.7398, val loss 2.7649\n",
      "step 30: train loss 2.6325, val loss 2.6507\n",
      "step 40: train loss 2.5795, val loss 2.6017\n",
      "step 50: train loss 2.5447, val loss 2.5721\n",
      "step 60: train loss 2.5249, val loss 2.5504\n",
      "step 70: train loss 2.5102, val loss 2.5348\n",
      "step 80: train loss 2.5021, val loss 2.5191\n",
      "step 90: train loss 2.4910, val loss 2.5120\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Initialize lists to store the loss values\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Define the number of iterations and the evaluation interval\n",
    "num_iterations = 100\n",
    "eval_interval = 10  # Adjust this to your desired evaluation frequency\n",
    "\n",
    "for iter in range(num_iterations):\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        # Store the loss values\n",
    "        train_losses.append(losses['train'])\n",
    "        val_losses.append(losses['val'])\n",
    "\n",
    "# Convert the lists of losses to PyTorch tensors\n",
    "train_losses_tensor = torch.tensor(train_losses)\n",
    "val_losses_tensor = torch.tensor(val_losses)\n",
    "\n",
    "# Compute the log10 of the loss values\n",
    "log_train_losses = torch.log10(train_losses_tensor)\n",
    "log_val_losses = torch.log10(val_losses_tensor)\n",
    "\n",
    "# Now plot the log-scaled loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(0, num_iterations, eval_interval), log_train_losses.numpy(), label='Train Loss')  # Convert to NumPy array for plotting\n",
    "plt.plot(range(0, num_iterations, eval_interval), log_val_losses.numpy(), label='Validation Loss')  # Convert to NumPy array for plotting\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log10 Loss')\n",
    "plt.title('Log10 Scaled Loss Over Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd960db9bd0>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(log_train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.4859, val loss 2.5087\n"
     ]
    }
   ],
   "source": [
    "for iter in range(100):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# function to get a batch of data for evaluation\n",
    "def get_evaluation_batch():\n",
    "    X, Y = get_batch('validation')  # assuming you have implemented get_batch for 'val' split\n",
    "    return X\n",
    "\n",
    "# function to plot activation distribution for transformer\n",
    "def plot_activation_distribution(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        X = get_evaluation_batch()\n",
    "        out, _ = model(X)  # assuming your model returns logits and loss\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            if isinstance(block, MultiHeadAttention):\n",
    "                plt.figure(figsize=(20, 4))\n",
    "                legends = []\n",
    "                for j, head in enumerate(block.heads):\n",
    "                    t = head(X)\n",
    "                    print('at block %d, head %d: mean %+.2f, std %.2f, saturated: %.2f%%' % (\n",
    "                        i, j, t.mean(), t.std(), (t.abs() > 0.97).float().mean() * 100))\n",
    "                    hy, hx = torch.histogram(t, density=True)\n",
    "                    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "                    legends.append(f'block {i}, head {j}')\n",
    "                plt.legend(legends)\n",
    "                plt.title('activation distribution - block %d' % i)\n",
    "                plt.show()\n",
    "        model.train()\n",
    "\n",
    "# function to plot gradient distribution for transformer\n",
    "def plot_gradient_distribution(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        X = get_evaluation_batch()\n",
    "        _, loss = model(X)\n",
    "        loss.backward()\n",
    "        for i, block in enumerate(model.blocks):\n",
    "            if isinstance(block, MultiHeadAttention):\n",
    "                plt.figure(figsize=(20, 4))\n",
    "                legends = []\n",
    "                for j, head in enumerate(block.heads):\n",
    "                    t = head(X).grad\n",
    "                    print('at block %d, head %d: mean %+f, std %e' % (i, j, t.mean(), t.std()))\n",
    "                    hy, hx = torch.histogram(t, density=True)\n",
    "                    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "                    legends.append(f'block {i}, head {j}')\n",
    "                plt.legend(legends)\n",
    "                plt.title('gradient distribution - block %d' % i)\n",
    "                plt.show()\n",
    "        model.train()\n",
    "\n",
    "# function to plot gradient distribution for parameters in transformer\n",
    "def plot_parameter_gradient_distribution(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        X = get_evaluation_batch()\n",
    "        _, loss = model(X)\n",
    "        loss.backward()\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        legends = []\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            if param.requires_grad and param.ndim == 2:\n",
    "                t = param.grad\n",
    "                print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (\n",
    "                    tuple(param.shape), t.mean(), t.std(), t.std() / param.std()))\n",
    "                hy, hx = torch.histogram(t, density=True)\n",
    "                plt.plot(hx[:-1].detach(), hy.detach())\n",
    "                legends.append(f'parameter {i}, {tuple(param.shape)}')\n",
    "        plt.legend(legends)\n",
    "        plt.title('parameter gradient distribution')\n",
    "        plt.show()\n",
    "\n",
    "# function to plot update to data ratio for parameters in transformer\n",
    "def plot_update_to_data_ratio(model, update_to_data_ratio):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "    for i, _ in enumerate(model.parameters()):\n",
    "        if i < len(update_to_data_ratio[0]) and i % 2 == 0:  # considering update ratio for every alternate parameter\n",
    "            plt.plot([update_to_data_ratio[j][i] for j in range(len(update_to_data_ratio))])\n",
    "            legends.append('param %d' % i)\n",
    "    plt.plot([0, len(update_to_data_ratio)], [-3, -3], 'k')  # indicate desired update ratio\n",
    "    plt.title('update to data ratio')\n",
    "    plt.legend(legends)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, _ = get_batch('val')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4194e+00,  5.8076e+00,  1.0911e+00,  ..., -2.1556e-01,\n",
       "           2.1234e+00, -1.1568e+00],\n",
       "         [-1.8366e+00, -1.7497e+00, -3.6225e+00,  ..., -2.4648e+00,\n",
       "           2.6041e+00, -1.8597e+00],\n",
       "         [ 1.0686e+00,  2.7166e+00, -7.3961e-01,  ..., -7.7444e-01,\n",
       "           7.6470e-01, -1.9132e+00],\n",
       "         ...,\n",
       "         [ 2.9132e-01,  2.0810e+00, -9.0999e-01,  ..., -3.4938e+00,\n",
       "           4.6142e-01, -2.6937e+00],\n",
       "         [ 2.1335e+00,  4.9496e+00, -6.5519e-01,  ..., -2.4621e+00,\n",
       "           4.0263e+00, -2.8865e+00],\n",
       "         [ 2.7994e+00,  6.5442e+00,  2.2630e+00,  ..., -2.2830e+00,\n",
       "          -7.2099e-01, -1.7383e+00]],\n",
       "\n",
       "        [[-3.8716e+00, -2.8788e+00, -4.7321e+00,  ..., -4.2474e+00,\n",
       "           2.9308e+00, -2.6291e+00],\n",
       "         [ 6.6647e-01,  4.1728e+00, -1.0635e+00,  ..., -3.0045e+00,\n",
       "           2.1208e+00, -3.8992e+00],\n",
       "         [ 1.7415e-03,  3.2647e+00, -1.3196e+00,  ..., -1.2860e+00,\n",
       "           3.6561e+00, -1.4794e+00],\n",
       "         ...,\n",
       "         [ 4.6115e+00,  5.4787e-02, -3.0399e+00,  ..., -3.7242e+00,\n",
       "          -5.3753e-01, -1.3642e+00],\n",
       "         [ 1.3744e+00,  2.5343e+00, -1.2706e+00,  ..., -1.8381e+00,\n",
       "           6.0486e-01, -3.5419e+00],\n",
       "         [ 1.2885e+00,  4.8097e+00,  1.7481e-01,  ..., -2.7100e+00,\n",
       "           3.6412e+00, -2.3803e+00]],\n",
       "\n",
       "        [[ 1.9773e+00,  5.1091e+00,  1.7607e+00,  ..., -2.4695e+00,\n",
       "           1.6926e+00, -2.2270e+00],\n",
       "         [ 2.5478e+00,  3.9849e+00,  6.3142e-03,  ..., -5.1366e-01,\n",
       "           8.6203e-01, -1.2611e+00],\n",
       "         [ 3.5585e+00,  6.6393e+00,  2.2775e+00,  ..., -1.7224e+00,\n",
       "           2.0651e+00, -1.5835e+00],\n",
       "         ...,\n",
       "         [-7.3234e-01,  9.5830e-01, -2.4529e+00,  ..., -3.6862e+00,\n",
       "           1.7946e-01, -9.2081e-01],\n",
       "         [ 1.0239e+00,  1.7682e+00, -2.6624e+00,  ..., -2.7515e+00,\n",
       "          -6.8164e-02, -2.9198e+00],\n",
       "         [-8.9247e-01,  1.0434e+00, -1.5537e+00,  ..., -2.3668e+00,\n",
       "          -3.9104e-01, -1.4943e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.7200e+00,  5.6180e+00,  1.5990e-01,  ..., -3.1052e+00,\n",
       "           1.0708e+00, -7.8094e-01],\n",
       "         [ 4.3133e+00,  6.0017e+00,  6.3796e-01,  ..., -1.4862e+00,\n",
       "           3.1390e+00, -1.3147e+00],\n",
       "         [ 4.8073e+00,  7.0969e+00,  2.3322e+00,  ..., -2.0121e+00,\n",
       "           1.9765e+00, -1.3131e+00],\n",
       "         ...,\n",
       "         [ 2.5763e+00,  5.3536e+00,  9.3374e-01,  ..., -3.5866e+00,\n",
       "           3.6918e-01, -1.7056e+00],\n",
       "         [-1.7157e+00, -1.2454e+00, -3.6673e+00,  ..., -2.5010e+00,\n",
       "           2.3761e+00, -2.8658e+00],\n",
       "         [-2.0592e-01,  3.4370e+00, -6.3234e-01,  ..., -1.2627e+00,\n",
       "           3.4443e+00, -2.2086e-01]],\n",
       "\n",
       "        [[ 3.1846e+00,  5.9302e+00,  1.0200e+00,  ..., -3.0253e+00,\n",
       "           2.5690e+00, -3.6176e-01],\n",
       "         [ 2.3724e+00,  4.7292e+00,  2.4547e-01,  ..., -1.8059e+00,\n",
       "           2.5871e+00, -1.7174e+00],\n",
       "         [ 1.0895e+00,  4.7640e+00, -4.1740e-02,  ..., -1.8435e+00,\n",
       "           1.1638e+00, -1.5335e+00],\n",
       "         ...,\n",
       "         [-2.7093e+00, -1.6556e+00, -3.3294e+00,  ..., -3.8733e+00,\n",
       "           1.8532e+00, -1.8079e+00],\n",
       "         [ 3.0629e+00,  5.5264e+00,  7.7839e-01,  ..., -2.1646e+00,\n",
       "           5.5750e-01, -2.5353e+00],\n",
       "         [-1.7496e+00,  7.3742e-01, -1.7770e+00,  ..., -1.1584e+00,\n",
       "          -3.0547e-03,  2.0215e+00]],\n",
       "\n",
       "        [[ 1.1269e+00,  2.7915e+00, -1.6265e+00,  ..., -1.7703e+00,\n",
       "           2.6334e-01,  2.2846e+00],\n",
       "         [ 3.2131e+00,  5.6021e+00,  6.2861e-01,  ..., -1.7852e+00,\n",
       "           2.4074e+00, -2.1168e+00],\n",
       "         [-2.4794e+00, -1.6622e+00, -3.1547e+00,  ..., -2.6693e+00,\n",
       "           1.9962e+00, -2.9278e+00],\n",
       "         ...,\n",
       "         [ 1.9465e+00,  4.9331e+00, -1.6654e-01,  ..., -3.8495e+00,\n",
       "           3.9216e+00, -1.9656e+00],\n",
       "         [-1.9637e+00, -1.4004e+00, -3.7077e+00,  ..., -2.4764e+00,\n",
       "           2.3442e+00, -2.9184e+00],\n",
       "         [-4.2229e-01,  3.3468e+00, -6.1790e-01,  ..., -1.1814e+00,\n",
       "           3.3981e+00, -2.2882e-01]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, _ = model(X)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "index: 1, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "index: 2, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "index: 3, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "index: 4, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "index: 5, block: Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i, block in enumerate(model.blocks):\n",
    "    print(f\"index: {i}, block: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BigramLanguageModel(\n",
      "  (token_embedding_table): Embedding(65, 384)\n",
      "  (position_embedding_table): Embedding(256, 384)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): Block(\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x Head(\n",
      "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n",
      "token_embedding_table Embedding(65, 384)\n",
      "position_embedding_table Embedding(256, 384)\n",
      "blocks Sequential(\n",
      "  (0): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (1): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (2): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (3): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (4): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (5): Block(\n",
      "    (sa): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-3): 4 x Head(\n",
      "          (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (ffwd): FeedFoward(\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (3): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "blocks.0 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.0.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.0.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.0.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.0.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.0.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.0.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.0.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.0.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.0.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.0.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.0.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.0.ffwd.net.1 ReLU()\n",
      "blocks.0.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.0.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.0.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.0.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.1 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.1.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.1.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.1.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.1.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.1.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.1.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.1.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.1.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.1.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.1.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.1.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.1.ffwd.net.1 ReLU()\n",
      "blocks.1.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.1.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.1.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.1.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.2 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.2.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.2.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.2.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.2.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.2.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.2.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.2.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.2.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.2.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.2.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.2.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.2.ffwd.net.1 ReLU()\n",
      "blocks.2.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.2.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.2.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.2.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.3 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.3.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.3.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.3.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.3.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.3.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.3.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.3.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.3.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.3.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.3.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.3.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.3.ffwd.net.1 ReLU()\n",
      "blocks.3.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.3.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.3.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.3.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.4 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.4.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.4.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.4.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.4.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.4.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.4.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.4.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.4.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.4.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.4.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.4.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.4.ffwd.net.1 ReLU()\n",
      "blocks.4.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.4.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.4.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.4.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.5 Block(\n",
      "  (sa): MultiHeadAttention(\n",
      "    (heads): ModuleList(\n",
      "      (0-3): 4 x Head(\n",
      "        (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (ffwd): FeedFoward(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "blocks.5.sa MultiHeadAttention(\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Head(\n",
      "      (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.sa.heads ModuleList(\n",
      "  (0-3): 4 x Head(\n",
      "    (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.5.sa.heads.0 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.sa.heads.0.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.0.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.0.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.0.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.5.sa.heads.1 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.sa.heads.1.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.1.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.1.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.1.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.5.sa.heads.2 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.sa.heads.2.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.2.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.2.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.2.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.5.sa.heads.3 Head(\n",
      "  (key): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (query): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (value): Linear(in_features=384, out_features=96, bias=False)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.sa.heads.3.key Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.3.query Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.3.value Linear(in_features=384, out_features=96, bias=False)\n",
      "blocks.5.sa.heads.3.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.5.sa.proj Linear(in_features=384, out_features=384, bias=True)\n",
      "blocks.5.sa.dropout Dropout(p=0.2, inplace=False)\n",
      "blocks.5.ffwd FeedFoward(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "blocks.5.ffwd.net Sequential(\n",
      "  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "  (3): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "blocks.5.ffwd.net.0 Linear(in_features=384, out_features=1536, bias=True)\n",
      "blocks.5.ffwd.net.1 ReLU()\n",
      "blocks.5.ffwd.net.2 Linear(in_features=1536, out_features=384, bias=True)\n",
      "blocks.5.ffwd.net.3 Dropout(p=0.2, inplace=False)\n",
      "blocks.5.ln1 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "blocks.5.ln2 LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "ln_f LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head Linear(in_features=384, out_features=65, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: token_embedding_table\n",
      "  weight_mean: 0.003120766021311283\n",
      "  weight_std: 0.9999886155128479\n",
      "  weight_max: 3.8206722736358643\n",
      "  weight_min: -4.246124744415283\n",
      "  weight_norm: 157.983154296875\n",
      "\n",
      "Layer: position_embedding_table\n",
      "  weight_mean: 0.0004400835314299911\n",
      "  weight_std: 0.9949462413787842\n",
      "  weight_max: 4.812939167022705\n",
      "  weight_min: -3.9327151775360107\n",
      "  weight_norm: 311.9485778808594\n",
      "\n",
      "Layer: blocks.0.sa.heads.0.key\n",
      "  weight_mean: 3.029516665264964e-05\n",
      "  weight_std: 0.029840251430869102\n",
      "  weight_max: 0.064152292907238\n",
      "  weight_min: -0.06318517029285431\n",
      "  weight_norm: 5.729254245758057\n",
      "\n",
      "Layer: blocks.0.sa.heads.0.query\n",
      "  weight_mean: -0.00016252027126029134\n",
      "  weight_std: 0.029893632978200912\n",
      "  weight_max: 0.06539029628038406\n",
      "  weight_min: -0.06603138148784637\n",
      "  weight_norm: 5.739584445953369\n",
      "\n",
      "Layer: blocks.0.sa.heads.0.value\n",
      "  weight_mean: 7.175927021307871e-05\n",
      "  weight_std: 0.029530446976423264\n",
      "  weight_max: 0.06426113098859787\n",
      "  weight_min: -0.06215154379606247\n",
      "  weight_norm: 5.669785499572754\n",
      "\n",
      "Layer: blocks.0.sa.heads.1.key\n",
      "  weight_mean: 0.00011995486420346424\n",
      "  weight_std: 0.02998209558427334\n",
      "  weight_max: 0.0656033307313919\n",
      "  weight_min: -0.0651555061340332\n",
      "  weight_norm: 5.756530284881592\n",
      "\n",
      "Layer: blocks.0.sa.heads.1.query\n",
      "  weight_mean: 0.0001706448820186779\n",
      "  weight_std: 0.03007517009973526\n",
      "  weight_max: 0.06583303213119507\n",
      "  weight_min: -0.06517518311738968\n",
      "  weight_norm: 5.774447441101074\n",
      "\n",
      "Layer: blocks.0.sa.heads.1.value\n",
      "  weight_mean: -0.00013453078281600028\n",
      "  weight_std: 0.029451517388224602\n",
      "  weight_max: 0.06095820292830467\n",
      "  weight_min: -0.0626082718372345\n",
      "  weight_norm: 5.6546735763549805\n",
      "\n",
      "Layer: blocks.0.sa.heads.2.key\n",
      "  weight_mean: 0.00016777405107859522\n",
      "  weight_std: 0.03000050224363804\n",
      "  weight_max: 0.06612443178892136\n",
      "  weight_min: -0.0685010701417923\n",
      "  weight_norm: 5.760108470916748\n",
      "\n",
      "Layer: blocks.0.sa.heads.2.query\n",
      "  weight_mean: -3.540424586390145e-05\n",
      "  weight_std: 0.030101830139756203\n",
      "  weight_max: 0.06544532626867294\n",
      "  weight_min: -0.06425188481807709\n",
      "  weight_norm: 5.779476642608643\n",
      "\n",
      "Layer: blocks.0.sa.heads.2.value\n",
      "  weight_mean: -2.229574602097273e-05\n",
      "  weight_std: 0.029492123052477837\n",
      "  weight_max: 0.06538671255111694\n",
      "  weight_min: -0.06003011763095856\n",
      "  weight_norm: 5.662412166595459\n",
      "\n",
      "Layer: blocks.0.sa.heads.3.key\n",
      "  weight_mean: -0.00024942023446783423\n",
      "  weight_std: 0.030075285583734512\n",
      "  weight_max: 0.06782900542020798\n",
      "  weight_min: -0.06696569919586182\n",
      "  weight_norm: 5.774575233459473\n",
      "\n",
      "Layer: blocks.0.sa.heads.3.query\n",
      "  weight_mean: 1.3229117030277848e-05\n",
      "  weight_std: 0.03011743351817131\n",
      "  weight_max: 0.07211323827505112\n",
      "  weight_min: -0.07007040828466415\n",
      "  weight_norm: 5.782469749450684\n",
      "\n",
      "Layer: blocks.0.sa.heads.3.value\n",
      "  weight_mean: 0.0002744556695688516\n",
      "  weight_std: 0.029664458706974983\n",
      "  weight_max: 0.060882098972797394\n",
      "  weight_min: -0.0653754323720932\n",
      "  weight_norm: 5.695743083953857\n",
      "\n",
      "Layer: blocks.0.sa.proj\n",
      "  weight_mean: -0.0001270500652026385\n",
      "  weight_std: 0.029496250674128532\n",
      "  weight_max: 0.061554912477731705\n",
      "  weight_min: -0.06392456591129303\n",
      "  weight_norm: 11.326626777648926\n",
      "  bias_mean: 0.0020767771638929844\n",
      "  bias_std: 0.029938558116555214\n",
      "  bias_max: 0.05062726512551308\n",
      "  bias_min: -0.05084417760372162\n",
      "  bias_norm: 0.5873207449913025\n",
      "\n",
      "Layer: blocks.0.ffwd.net.0\n",
      "  weight_mean: -1.5868399714236148e-05\n",
      "  weight_std: 0.029621120542287827\n",
      "  weight_max: 0.06891041994094849\n",
      "  weight_min: -0.06830936670303345\n",
      "  weight_norm: 22.749004364013672\n",
      "  bias_mean: -0.0008928670431487262\n",
      "  bias_std: 0.029322974383831024\n",
      "  bias_max: 0.05272391811013222\n",
      "  bias_min: -0.05330965295433998\n",
      "  bias_norm: 1.1493799686431885\n",
      "\n",
      "Layer: blocks.0.ffwd.net.2\n",
      "  weight_mean: -5.839904770255089e-05\n",
      "  weight_std: 0.014818524941802025\n",
      "  weight_max: 0.03602592274546623\n",
      "  weight_min: -0.035821009427309036\n",
      "  weight_norm: 11.380706787109375\n",
      "  bias_mean: 0.00034697767114266753\n",
      "  bias_std: 0.014494339004158974\n",
      "  bias_max: 0.025629181414842606\n",
      "  bias_min: -0.025522498413920403\n",
      "  bias_norm: 0.283741295337677\n",
      "\n",
      "Layer: blocks.0.ln1\n",
      "  weight_mean: 1.0004870891571045\n",
      "  weight_std: 0.0035170521587133408\n",
      "  weight_max: 1.0186622142791748\n",
      "  weight_min: 0.9909523129463196\n",
      "  weight_norm: 19.60558319091797\n",
      "  bias_mean: 4.462290235096589e-05\n",
      "  bias_std: 0.0013070119312033057\n",
      "  bias_max: 0.003847011597827077\n",
      "  bias_min: -0.004374074749648571\n",
      "  bias_norm: 0.025593668222427368\n",
      "\n",
      "Layer: blocks.0.ln2\n",
      "  weight_mean: 0.9987428188323975\n",
      "  weight_std: 0.0034865152556449175\n",
      "  weight_max: 1.010143756866455\n",
      "  weight_min: 0.9909220933914185\n",
      "  weight_norm: 19.571401596069336\n",
      "  bias_mean: 4.337570135248825e-05\n",
      "  bias_std: 0.0016747654881328344\n",
      "  bias_max: 0.0064276037737727165\n",
      "  bias_min: -0.005157128442078829\n",
      "  bias_norm: 0.03278682753443718\n",
      "\n",
      "Layer: blocks.1.sa.heads.0.key\n",
      "  weight_mean: 8.69065843289718e-05\n",
      "  weight_std: 0.029859567061066628\n",
      "  weight_max: 0.06475105881690979\n",
      "  weight_min: -0.0633687749505043\n",
      "  weight_norm: 5.7329840660095215\n",
      "\n",
      "Layer: blocks.1.sa.heads.0.query\n",
      "  weight_mean: 0.00030349724693223834\n",
      "  weight_std: 0.029828161001205444\n",
      "  weight_max: 0.0654151514172554\n",
      "  weight_min: -0.06768238544464111\n",
      "  weight_norm: 5.727225303649902\n",
      "\n",
      "Layer: blocks.1.sa.heads.0.value\n",
      "  weight_mean: 7.728974014753476e-05\n",
      "  weight_std: 0.029445551335811615\n",
      "  weight_max: 0.060215696692466736\n",
      "  weight_min: -0.05859692767262459\n",
      "  weight_norm: 5.653488636016846\n",
      "\n",
      "Layer: blocks.1.sa.heads.1.key\n",
      "  weight_mean: 0.00016763231542427093\n",
      "  weight_std: 0.029981980100274086\n",
      "  weight_max: 0.06404215842485428\n",
      "  weight_min: -0.0653953105211258\n",
      "  weight_norm: 5.756552219390869\n",
      "\n",
      "Layer: blocks.1.sa.heads.1.query\n",
      "  weight_mean: -7.751184602966532e-05\n",
      "  weight_std: 0.029931984841823578\n",
      "  weight_max: 0.06840141117572784\n",
      "  weight_min: -0.06526866555213928\n",
      "  weight_norm: 5.746882438659668\n",
      "\n",
      "Layer: blocks.1.sa.heads.1.value\n",
      "  weight_mean: 0.00025205753627233207\n",
      "  weight_std: 0.029430901631712914\n",
      "  weight_max: 0.06293043494224548\n",
      "  weight_min: -0.05779633671045303\n",
      "  weight_norm: 5.6508636474609375\n",
      "\n",
      "Layer: blocks.1.sa.heads.2.key\n",
      "  weight_mean: 0.00010980403021676466\n",
      "  weight_std: 0.029829809442162514\n",
      "  weight_max: 0.06535975635051727\n",
      "  weight_min: -0.06317209452390671\n",
      "  weight_norm: 5.7272844314575195\n",
      "\n",
      "Layer: blocks.1.sa.heads.2.query\n",
      "  weight_mean: -5.807659181300551e-05\n",
      "  weight_std: 0.029892634600400925\n",
      "  weight_max: 0.06704262644052505\n",
      "  weight_min: -0.06779683381319046\n",
      "  weight_norm: 5.73931884765625\n",
      "\n",
      "Layer: blocks.1.sa.heads.2.value\n",
      "  weight_mean: -0.0001624115539016202\n",
      "  weight_std: 0.029448376968503\n",
      "  weight_max: 0.0626722052693367\n",
      "  weight_min: -0.05953490734100342\n",
      "  weight_norm: 5.654098033905029\n",
      "\n",
      "Layer: blocks.1.sa.heads.3.key\n",
      "  weight_mean: 0.0002045310684479773\n",
      "  weight_std: 0.029791338369250298\n",
      "  weight_max: 0.06514673680067062\n",
      "  weight_min: -0.0653720498085022\n",
      "  weight_norm: 5.719994068145752\n",
      "\n",
      "Layer: blocks.1.sa.heads.3.query\n",
      "  weight_mean: -9.01901294128038e-05\n",
      "  weight_std: 0.029775500297546387\n",
      "  weight_max: 0.06385651230812073\n",
      "  weight_min: -0.06442790478467941\n",
      "  weight_norm: 5.71684455871582\n",
      "\n",
      "Layer: blocks.1.sa.heads.3.value\n",
      "  weight_mean: 4.826888834941201e-05\n",
      "  weight_std: 0.029495393857359886\n",
      "  weight_max: 0.058337680995464325\n",
      "  weight_min: -0.05699462443590164\n",
      "  weight_norm: 5.663045883178711\n",
      "\n",
      "Layer: blocks.1.sa.proj\n",
      "  weight_mean: -2.6515443096286617e-05\n",
      "  weight_std: 0.02948158234357834\n",
      "  weight_max: 0.06201382726430893\n",
      "  weight_min: -0.0614047646522522\n",
      "  weight_norm: 11.320893287658691\n",
      "  bias_mean: 2.737242539296858e-05\n",
      "  bias_std: 0.02945089153945446\n",
      "  bias_max: 0.04990367218852043\n",
      "  bias_min: -0.050961948931217194\n",
      "  bias_norm: 0.5763655304908752\n",
      "\n",
      "Layer: blocks.1.ffwd.net.0\n",
      "  weight_mean: 3.369757178006694e-05\n",
      "  weight_std: 0.029636777937412262\n",
      "  weight_max: 0.06637014448642731\n",
      "  weight_min: -0.06743361055850983\n",
      "  weight_norm: 22.76104164123535\n",
      "  bias_mean: -0.0012250489089637995\n",
      "  bias_std: 0.028361136093735695\n",
      "  bias_max: 0.05020633339881897\n",
      "  bias_min: -0.05415762960910797\n",
      "  bias_norm: 1.1121997833251953\n",
      "\n",
      "Layer: blocks.1.ffwd.net.2\n",
      "  weight_mean: -6.732910696882755e-05\n",
      "  weight_std: 0.01481854822486639\n",
      "  weight_max: 0.033568836748600006\n",
      "  weight_min: -0.03569086268544197\n",
      "  weight_norm: 11.380753517150879\n",
      "  bias_mean: 9.63152851909399e-05\n",
      "  bias_std: 0.01483365148305893\n",
      "  bias_max: 0.025201546028256416\n",
      "  bias_min: -0.025692356750369072\n",
      "  bias_norm: 0.29030641913414\n",
      "\n",
      "Layer: blocks.1.ln1\n",
      "  weight_mean: 0.9994233846664429\n",
      "  weight_std: 0.0021808668971061707\n",
      "  weight_max: 1.0111918449401855\n",
      "  weight_min: 0.9917287230491638\n",
      "  weight_norm: 19.584665298461914\n",
      "  bias_mean: 9.558687452226877e-06\n",
      "  bias_std: 0.001157684950158\n",
      "  bias_max: 0.0037882791366428137\n",
      "  bias_min: -0.0035884759854525328\n",
      "  bias_norm: 0.022657113149762154\n",
      "\n",
      "Layer: blocks.1.ln2\n",
      "  weight_mean: 0.9988147616386414\n",
      "  weight_std: 0.0034051472321152687\n",
      "  weight_max: 1.0145370960235596\n",
      "  weight_min: 0.992203950881958\n",
      "  weight_norm: 19.572805404663086\n",
      "  bias_mean: -2.2148136849864386e-05\n",
      "  bias_std: 0.00147613778244704\n",
      "  bias_max: 0.004275530111044645\n",
      "  bias_min: -0.005524803884327412\n",
      "  bias_norm: 0.028891846537590027\n",
      "\n",
      "Layer: blocks.2.sa.heads.0.key\n",
      "  weight_mean: 6.057426071492955e-05\n",
      "  weight_std: 0.029812462627887726\n",
      "  weight_max: 0.06258713454008102\n",
      "  weight_min: -0.06236627697944641\n",
      "  weight_norm: 5.7239274978637695\n",
      "\n",
      "Layer: blocks.2.sa.heads.0.query\n",
      "  weight_mean: -0.0001312789536314085\n",
      "  weight_std: 0.0298190675675869\n",
      "  weight_max: 0.06587239354848862\n",
      "  weight_min: -0.06714554876089096\n",
      "  weight_norm: 5.725238800048828\n",
      "\n",
      "Layer: blocks.2.sa.heads.0.value\n",
      "  weight_mean: 0.0002449776802677661\n",
      "  weight_std: 0.029490957036614418\n",
      "  weight_max: 0.057584211230278015\n",
      "  weight_min: -0.061788201332092285\n",
      "  weight_norm: 5.662382125854492\n",
      "\n",
      "Layer: blocks.2.sa.heads.1.key\n",
      "  weight_mean: 0.0002750725543592125\n",
      "  weight_std: 0.029941821470856667\n",
      "  weight_max: 0.06509266048669815\n",
      "  weight_min: -0.06435389071702957\n",
      "  weight_norm: 5.74899435043335\n",
      "\n",
      "Layer: blocks.2.sa.heads.1.query\n",
      "  weight_mean: 4.327315400587395e-05\n",
      "  weight_std: 0.02994384989142418\n",
      "  weight_max: 0.06380955874919891\n",
      "  weight_min: -0.06707930564880371\n",
      "  weight_norm: 5.749147415161133\n",
      "\n",
      "Layer: blocks.2.sa.heads.1.value\n",
      "  weight_mean: -0.00012363280984573066\n",
      "  weight_std: 0.029504524543881416\n",
      "  weight_max: 0.05832509696483612\n",
      "  weight_min: -0.056504055857658386\n",
      "  weight_norm: 5.664841651916504\n",
      "\n",
      "Layer: blocks.2.sa.heads.2.key\n",
      "  weight_mean: -9.484533075010404e-05\n",
      "  weight_std: 0.02977066859602928\n",
      "  weight_max: 0.06140942871570587\n",
      "  weight_min: -0.06220032274723053\n",
      "  weight_norm: 5.715920448303223\n",
      "\n",
      "Layer: blocks.2.sa.heads.2.query\n",
      "  weight_mean: 0.0002096776879625395\n",
      "  weight_std: 0.029811853542923927\n",
      "  weight_max: 0.06275845319032669\n",
      "  weight_min: -0.06328646838665009\n",
      "  weight_norm: 5.723939895629883\n",
      "\n",
      "Layer: blocks.2.sa.heads.2.value\n",
      "  weight_mean: -2.490419319656212e-05\n",
      "  weight_std: 0.029530977830290794\n",
      "  weight_max: 0.057538338005542755\n",
      "  weight_min: -0.05733703076839447\n",
      "  weight_norm: 5.669873237609863\n",
      "\n",
      "Layer: blocks.2.sa.heads.3.key\n",
      "  weight_mean: -0.000432745146099478\n",
      "  weight_std: 0.029708974063396454\n",
      "  weight_max: 0.06277570873498917\n",
      "  weight_min: -0.06356360763311386\n",
      "  weight_norm: 5.704650402069092\n",
      "\n",
      "Layer: blocks.2.sa.heads.3.query\n",
      "  weight_mean: -1.4166567780193873e-05\n",
      "  weight_std: 0.029787665233016014\n",
      "  weight_max: 0.062155064195394516\n",
      "  weight_min: -0.06162514537572861\n",
      "  weight_norm: 5.7191548347473145\n",
      "\n",
      "Layer: blocks.2.sa.heads.3.value\n",
      "  weight_mean: -9.909228538163006e-05\n",
      "  weight_std: 0.029483776539564133\n",
      "  weight_max: 0.05740790069103241\n",
      "  weight_min: -0.05835314467549324\n",
      "  weight_norm: 5.660839557647705\n",
      "\n",
      "Layer: blocks.2.sa.proj\n",
      "  weight_mean: -0.0001215934898937121\n",
      "  weight_std: 0.029435938224196434\n",
      "  weight_max: 0.059467531740665436\n",
      "  weight_min: -0.059042155742645264\n",
      "  weight_norm: 11.303458213806152\n",
      "  bias_mean: 0.003013035049661994\n",
      "  bias_std: 0.029230637475848198\n",
      "  bias_max: 0.05094773694872856\n",
      "  bias_min: -0.05030721798539162\n",
      "  bias_norm: 0.5750937461853027\n",
      "\n",
      "Layer: blocks.2.ffwd.net.0\n",
      "  weight_mean: -6.156536983326077e-05\n",
      "  weight_std: 0.029614713042974472\n",
      "  weight_max: 0.06618412584066391\n",
      "  weight_min: -0.06710943579673767\n",
      "  weight_norm: 22.744129180908203\n",
      "  bias_mean: -0.0018221728969365358\n",
      "  bias_std: 0.029087843373417854\n",
      "  bias_max: 0.05118124559521675\n",
      "  bias_min: -0.05332008749246597\n",
      "  bias_norm: 1.141870141029358\n",
      "\n",
      "Layer: blocks.2.ffwd.net.2\n",
      "  weight_mean: -6.98134463164024e-05\n",
      "  weight_std: 0.014821733348071575\n",
      "  weight_max: 0.03631984815001488\n",
      "  weight_min: -0.03471430391073227\n",
      "  weight_norm: 11.383208274841309\n",
      "  bias_mean: -0.0004536262131296098\n",
      "  bias_std: 0.015028947032988071\n",
      "  bias_max: 0.025151852518320084\n",
      "  bias_min: -0.025707870721817017\n",
      "  bias_norm: 0.2942565977573395\n",
      "\n",
      "Layer: blocks.2.ln1\n",
      "  weight_mean: 0.9988743662834167\n",
      "  weight_std: 0.0018293068278580904\n",
      "  weight_max: 1.0083129405975342\n",
      "  weight_min: 0.9914214611053467\n",
      "  weight_norm: 19.57389259338379\n",
      "  bias_mean: -1.005612557491986e-05\n",
      "  bias_std: 0.0010599115630611777\n",
      "  bias_max: 0.004855695646256208\n",
      "  bias_min: -0.005767796654254198\n",
      "  bias_norm: 0.02074381522834301\n",
      "\n",
      "Layer: blocks.2.ln2\n",
      "  weight_mean: 0.9985876083374023\n",
      "  weight_std: 0.0028590706642717123\n",
      "  weight_max: 1.0083400011062622\n",
      "  weight_min: 0.9918980002403259\n",
      "  weight_norm: 19.568321228027344\n",
      "  bias_mean: 6.579079490620643e-05\n",
      "  bias_std: 0.001403791713528335\n",
      "  bias_max: 0.004677964840084314\n",
      "  bias_min: -0.005334813147783279\n",
      "  bias_norm: 0.02750297822058201\n",
      "\n",
      "Layer: blocks.3.sa.heads.0.key\n",
      "  weight_mean: -9.20715876873146e-08\n",
      "  weight_std: 0.029765469953417778\n",
      "  weight_max: 0.06157490611076355\n",
      "  weight_min: -0.06228449568152428\n",
      "  weight_norm: 5.714892387390137\n",
      "\n",
      "Layer: blocks.3.sa.heads.0.query\n",
      "  weight_mean: 9.826994937611744e-05\n",
      "  weight_std: 0.029785623773932457\n",
      "  weight_max: 0.06299187242984772\n",
      "  weight_min: -0.06498216837644577\n",
      "  weight_norm: 5.7187933921813965\n",
      "\n",
      "Layer: blocks.3.sa.heads.0.value\n",
      "  weight_mean: -0.00014809575804974884\n",
      "  weight_std: 0.02953587844967842\n",
      "  weight_max: 0.05610743910074234\n",
      "  weight_min: -0.05616798996925354\n",
      "  weight_norm: 5.670883655548096\n",
      "\n",
      "Layer: blocks.3.sa.heads.1.key\n",
      "  weight_mean: 4.656830060412176e-05\n",
      "  weight_std: 0.029819365590810776\n",
      "  weight_max: 0.0643581673502922\n",
      "  weight_min: -0.06389791518449783\n",
      "  weight_norm: 5.725247859954834\n",
      "\n",
      "Layer: blocks.3.sa.heads.1.query\n",
      "  weight_mean: 3.453651152085513e-05\n",
      "  weight_std: 0.02974749729037285\n",
      "  weight_max: 0.06573255360126495\n",
      "  weight_min: -0.060836922377347946\n",
      "  weight_norm: 5.7114458084106445\n",
      "\n",
      "Layer: blocks.3.sa.heads.1.value\n",
      "  weight_mean: -0.0001201714767375961\n",
      "  weight_std: 0.029487164691090584\n",
      "  weight_max: 0.055689677596092224\n",
      "  weight_min: -0.05577140673995018\n",
      "  weight_norm: 5.661506175994873\n",
      "\n",
      "Layer: blocks.3.sa.heads.2.key\n",
      "  weight_mean: 0.0002397295756964013\n",
      "  weight_std: 0.02963448129594326\n",
      "  weight_max: 0.06218327581882477\n",
      "  weight_min: -0.06373362988233566\n",
      "  weight_norm: 5.689929485321045\n",
      "\n",
      "Layer: blocks.3.sa.heads.2.query\n",
      "  weight_mean: 0.0003148588293697685\n",
      "  weight_std: 0.029665116220712662\n",
      "  weight_max: 0.061345651745796204\n",
      "  weight_min: -0.06259755790233612\n",
      "  weight_norm: 5.6959452629089355\n",
      "\n",
      "Layer: blocks.3.sa.heads.2.value\n",
      "  weight_mean: 0.00028814550023525953\n",
      "  weight_std: 0.029485318809747696\n",
      "  weight_max: 0.05589911341667175\n",
      "  weight_min: -0.05666574835777283\n",
      "  weight_norm: 5.661374568939209\n",
      "\n",
      "Layer: blocks.3.sa.heads.3.key\n",
      "  weight_mean: -5.048654566053301e-05\n",
      "  weight_std: 0.02979511022567749\n",
      "  weight_max: 0.06250840425491333\n",
      "  weight_min: -0.06829491257667542\n",
      "  weight_norm: 5.7205915451049805\n",
      "\n",
      "Layer: blocks.3.sa.heads.3.query\n",
      "  weight_mean: 4.987660940969363e-05\n",
      "  weight_std: 0.02978931926190853\n",
      "  weight_max: 0.06635510921478271\n",
      "  weight_min: -0.06076548621058464\n",
      "  weight_norm: 5.719479560852051\n",
      "\n",
      "Layer: blocks.3.sa.heads.3.value\n",
      "  weight_mean: 0.00029243621975183487\n",
      "  weight_std: 0.02940460667014122\n",
      "  weight_max: 0.05655760318040848\n",
      "  weight_min: -0.05529871955513954\n",
      "  weight_norm: 5.6458868980407715\n",
      "\n",
      "Layer: blocks.3.sa.proj\n",
      "  weight_mean: 0.00010903021757258102\n",
      "  weight_std: 0.02948881685733795\n",
      "  weight_max: 0.06116608530282974\n",
      "  weight_min: -0.059522103518247604\n",
      "  weight_norm: 11.323744773864746\n",
      "  bias_mean: 0.00038583303103223443\n",
      "  bias_std: 0.02877253293991089\n",
      "  bias_max: 0.050323422998189926\n",
      "  bias_min: -0.050852369517087936\n",
      "  bias_norm: 0.563140332698822\n",
      "\n",
      "Layer: blocks.3.ffwd.net.0\n",
      "  weight_mean: 5.260135731077753e-05\n",
      "  weight_std: 0.02961125411093235\n",
      "  weight_max: 0.0676327720284462\n",
      "  weight_min: -0.06693625450134277\n",
      "  weight_norm: 22.741458892822266\n",
      "  bias_mean: -0.00014467511209659278\n",
      "  bias_std: 0.028480898588895798\n",
      "  bias_max: 0.05229542776942253\n",
      "  bias_min: -0.05291113257408142\n",
      "  bias_norm: 1.1158696413040161\n",
      "\n",
      "Layer: blocks.3.ffwd.net.2\n",
      "  weight_mean: -6.743529957020655e-05\n",
      "  weight_std: 0.014812005683779716\n",
      "  weight_max: 0.036797501146793365\n",
      "  weight_min: -0.03668231889605522\n",
      "  weight_norm: 11.375728607177734\n",
      "  bias_mean: -0.00030127764330245554\n",
      "  bias_std: 0.014559777453541756\n",
      "  bias_max: 0.025445284321904182\n",
      "  bias_min: -0.025358449667692184\n",
      "  bias_norm: 0.28500160574913025\n",
      "\n",
      "Layer: blocks.3.ln1\n",
      "  weight_mean: 0.9987430572509766\n",
      "  weight_std: 0.0015425921883434057\n",
      "  weight_max: 1.0060042142868042\n",
      "  weight_min: 0.9942261576652527\n",
      "  weight_norm: 19.571311950683594\n",
      "  bias_mean: -3.223060048185289e-06\n",
      "  bias_std: 0.001019102637656033\n",
      "  bias_max: 0.003457541111856699\n",
      "  bias_min: -0.005209912545979023\n",
      "  bias_norm: 0.0199443306773901\n",
      "\n",
      "Layer: blocks.3.ln2\n",
      "  weight_mean: 0.99864262342453\n",
      "  weight_std: 0.0028709648177027702\n",
      "  weight_max: 1.0091646909713745\n",
      "  weight_min: 0.9881615042686462\n",
      "  weight_norm: 19.569398880004883\n",
      "  bias_mean: -3.025946716661565e-05\n",
      "  bias_std: 0.0014620618894696236\n",
      "  bias_max: 0.006464006844907999\n",
      "  bias_min: -0.005155942868441343\n",
      "  bias_norm: 0.028619257733225822\n",
      "\n",
      "Layer: blocks.4.sa.heads.0.key\n",
      "  weight_mean: -0.00019216838700231165\n",
      "  weight_std: 0.029748106375336647\n",
      "  weight_max: 0.06398947536945343\n",
      "  weight_min: -0.06467637419700623\n",
      "  weight_norm: 5.7116780281066895\n",
      "\n",
      "Layer: blocks.4.sa.heads.0.query\n",
      "  weight_mean: -8.384289685636759e-05\n",
      "  weight_std: 0.029819615185260773\n",
      "  weight_max: 0.06399965286254883\n",
      "  weight_min: -0.06598074734210968\n",
      "  weight_norm: 5.725311279296875\n",
      "\n",
      "Layer: blocks.4.sa.heads.0.value\n",
      "  weight_mean: 0.00014226400526240468\n",
      "  weight_std: 0.029373737052083015\n",
      "  weight_max: 0.055733855813741684\n",
      "  weight_min: -0.056587569415569305\n",
      "  weight_norm: 5.639747619628906\n",
      "\n",
      "Layer: blocks.4.sa.heads.1.key\n",
      "  weight_mean: 1.403415808454156e-05\n",
      "  weight_std: 0.029808083549141884\n",
      "  weight_max: 0.06334695965051651\n",
      "  weight_min: -0.06229612976312637\n",
      "  weight_norm: 5.723074913024902\n",
      "\n",
      "Layer: blocks.4.sa.heads.1.query\n",
      "  weight_mean: 0.000168421640410088\n",
      "  weight_std: 0.02974356897175312\n",
      "  weight_max: 0.0638372078537941\n",
      "  weight_min: -0.06146584451198578\n",
      "  weight_norm: 5.710778713226318\n",
      "\n",
      "Layer: blocks.4.sa.heads.1.value\n",
      "  weight_mean: -9.671360749052837e-05\n",
      "  weight_std: 0.029392503201961517\n",
      "  weight_max: 0.05794691666960716\n",
      "  weight_min: -0.0552012100815773\n",
      "  weight_norm: 5.643314838409424\n",
      "\n",
      "Layer: blocks.4.sa.heads.2.key\n",
      "  weight_mean: 0.00033624909701757133\n",
      "  weight_std: 0.029870057478547096\n",
      "  weight_max: 0.06465274840593338\n",
      "  weight_min: -0.06031012907624245\n",
      "  weight_norm: 5.735336780548096\n",
      "\n",
      "Layer: blocks.4.sa.heads.2.query\n",
      "  weight_mean: 0.00031063525239005685\n",
      "  weight_std: 0.029810544103384018\n",
      "  weight_max: 0.06364135444164276\n",
      "  weight_min: -0.06442452222108841\n",
      "  weight_norm: 5.723857402801514\n",
      "\n",
      "Layer: blocks.4.sa.heads.2.value\n",
      "  weight_mean: 0.00010281158029101789\n",
      "  weight_std: 0.029444510117173195\n",
      "  weight_max: 0.05702575296163559\n",
      "  weight_min: -0.055955447256565094\n",
      "  weight_norm: 5.653303623199463\n",
      "\n",
      "Layer: blocks.4.sa.heads.3.key\n",
      "  weight_mean: 0.0002912644122261554\n",
      "  weight_std: 0.029856860637664795\n",
      "  weight_max: 0.06289011985063553\n",
      "  weight_min: -0.06402810662984848\n",
      "  weight_norm: 5.732712268829346\n",
      "\n",
      "Layer: blocks.4.sa.heads.3.query\n",
      "  weight_mean: 0.00013699168630409986\n",
      "  weight_std: 0.02971561625599861\n",
      "  weight_max: 0.061681050807237625\n",
      "  weight_min: -0.06344404071569443\n",
      "  weight_norm: 5.705381393432617\n",
      "\n",
      "Layer: blocks.4.sa.heads.3.value\n",
      "  weight_mean: -0.0003703524707816541\n",
      "  weight_std: 0.0294922087341547\n",
      "  weight_max: 0.05469466745853424\n",
      "  weight_min: -0.05500528961420059\n",
      "  weight_norm: 5.6628737449646\n",
      "\n",
      "Layer: blocks.4.sa.proj\n",
      "  weight_mean: 3.2794199796626344e-05\n",
      "  weight_std: 0.02945903316140175\n",
      "  weight_max: 0.06062585115432739\n",
      "  weight_min: -0.05977736413478851\n",
      "  weight_norm: 11.312237739562988\n",
      "  bias_mean: 0.00015705525584053248\n",
      "  bias_std: 0.029339207336306572\n",
      "  bias_max: 0.051092032343149185\n",
      "  bias_min: -0.05258415639400482\n",
      "  bias_norm: 0.5741878151893616\n",
      "\n",
      "Layer: blocks.4.ffwd.net.0\n",
      "  weight_mean: 1.3737037079408765e-05\n",
      "  weight_std: 0.02959301695227623\n",
      "  weight_max: 0.06541257351636887\n",
      "  weight_min: -0.06609614193439484\n",
      "  weight_norm: 22.727420806884766\n",
      "  bias_mean: -0.001291179098188877\n",
      "  bias_std: 0.029595401138067245\n",
      "  bias_max: 0.05128704011440277\n",
      "  bias_min: -0.05316077172756195\n",
      "  bias_norm: 1.1606241464614868\n",
      "\n",
      "Layer: blocks.4.ffwd.net.2\n",
      "  weight_mean: -7.687575271120295e-05\n",
      "  weight_std: 0.014808306470513344\n",
      "  weight_max: 0.03661850467324257\n",
      "  weight_min: -0.03916357830166817\n",
      "  weight_norm: 11.372922897338867\n",
      "  bias_mean: 0.0016620734240859747\n",
      "  bias_std: 0.014457667246460915\n",
      "  bias_max: 0.026034723967313766\n",
      "  bias_min: -0.025872156023979187\n",
      "  bias_norm: 0.28481051325798035\n",
      "\n",
      "Layer: blocks.4.ln1\n",
      "  weight_mean: 0.9986602067947388\n",
      "  weight_std: 0.0016572469612583518\n",
      "  weight_max: 1.0043556690216064\n",
      "  weight_min: 0.993558406829834\n",
      "  weight_norm: 19.56968879699707\n",
      "  bias_mean: -2.9565384465968236e-05\n",
      "  bias_std: 0.0011844886466860771\n",
      "  bias_max: 0.0033336507622152567\n",
      "  bias_min: -0.005059391725808382\n",
      "  bias_norm: 0.023188138380646706\n",
      "\n",
      "Layer: blocks.4.ln2\n",
      "  weight_mean: 0.9986178874969482\n",
      "  weight_std: 0.0028109275735914707\n",
      "  weight_max: 1.0096275806427002\n",
      "  weight_min: 0.9915279150009155\n",
      "  weight_norm: 19.568910598754883\n",
      "  bias_mean: 8.960235390986782e-06\n",
      "  bias_std: 0.0015558357117697597\n",
      "  bias_max: 0.004590000491589308\n",
      "  bias_min: -0.005377150606364012\n",
      "  bias_norm: 0.030448811128735542\n",
      "\n",
      "Layer: blocks.5.sa.heads.0.key\n",
      "  weight_mean: -0.0001763861655490473\n",
      "  weight_std: 0.029750745743513107\n",
      "  weight_max: 0.06223293021321297\n",
      "  weight_min: -0.061058297753334045\n",
      "  weight_norm: 5.712166786193848\n",
      "\n",
      "Layer: blocks.5.sa.heads.0.query\n",
      "  weight_mean: 1.2312239960010629e-05\n",
      "  weight_std: 0.0296533964574337\n",
      "  weight_max: 0.06379752606153488\n",
      "  weight_min: -0.06538877636194229\n",
      "  weight_norm: 5.693375110626221\n",
      "\n",
      "Layer: blocks.5.sa.heads.0.value\n",
      "  weight_mean: 7.654976798221469e-05\n",
      "  weight_std: 0.0295195821672678\n",
      "  weight_max: 0.055864762514829636\n",
      "  weight_min: -0.05504702776670456\n",
      "  weight_norm: 5.6677021980285645\n",
      "\n",
      "Layer: blocks.5.sa.heads.1.key\n",
      "  weight_mean: 6.940454477444291e-05\n",
      "  weight_std: 0.0297910925000906\n",
      "  weight_max: 0.06175088509917259\n",
      "  weight_min: -0.06390158832073212\n",
      "  weight_norm: 5.719827651977539\n",
      "\n",
      "Layer: blocks.5.sa.heads.1.query\n",
      "  weight_mean: 8.217411232180893e-05\n",
      "  weight_std: 0.029749484732747078\n",
      "  weight_max: 0.06198927387595177\n",
      "  weight_min: -0.061998821794986725\n",
      "  weight_norm: 5.711845397949219\n",
      "\n",
      "Layer: blocks.5.sa.heads.1.value\n",
      "  weight_mean: 6.596971797989681e-05\n",
      "  weight_std: 0.0295563917607069\n",
      "  weight_max: 0.05672885477542877\n",
      "  weight_min: -0.05510108917951584\n",
      "  weight_norm: 5.674764633178711\n",
      "\n",
      "Layer: blocks.5.sa.heads.2.key\n",
      "  weight_mean: -0.00027186310035176575\n",
      "  weight_std: 0.0295894555747509\n",
      "  weight_max: 0.06440593302249908\n",
      "  weight_min: -0.0685635581612587\n",
      "  weight_norm: 5.681338310241699\n",
      "\n",
      "Layer: blocks.5.sa.heads.2.query\n",
      "  weight_mean: -0.0001907899131765589\n",
      "  weight_std: 0.029773171991109848\n",
      "  weight_max: 0.06293732672929764\n",
      "  weight_min: -0.0625358521938324\n",
      "  weight_norm: 5.716489315032959\n",
      "\n",
      "Layer: blocks.5.sa.heads.2.value\n",
      "  weight_mean: -5.6777407735353336e-05\n",
      "  weight_std: 0.029451949521899223\n",
      "  weight_max: 0.05719595029950142\n",
      "  weight_min: -0.0553886778652668\n",
      "  weight_norm: 5.654707908630371\n",
      "\n",
      "Layer: blocks.5.sa.heads.3.key\n",
      "  weight_mean: 6.055242920410819e-06\n",
      "  weight_std: 0.02971125766634941\n",
      "  weight_max: 0.06261478364467621\n",
      "  weight_min: -0.06007947400212288\n",
      "  weight_norm: 5.704483985900879\n",
      "\n",
      "Layer: blocks.5.sa.heads.3.query\n",
      "  weight_mean: 6.979417230468243e-05\n",
      "  weight_std: 0.029652109369635582\n",
      "  weight_max: 0.06087987869977951\n",
      "  weight_min: -0.06133366376161575\n",
      "  weight_norm: 5.693143844604492\n",
      "\n",
      "Layer: blocks.5.sa.heads.3.value\n",
      "  weight_mean: 9.653758752392605e-05\n",
      "  weight_std: 0.029529696330428123\n",
      "  weight_max: 0.055611636489629745\n",
      "  weight_min: -0.05626349896192551\n",
      "  weight_norm: 5.6696553230285645\n",
      "\n",
      "Layer: blocks.5.sa.proj\n",
      "  weight_mean: -7.533541793236509e-05\n",
      "  weight_std: 0.02947768196463585\n",
      "  weight_max: 0.056396882981061935\n",
      "  weight_min: -0.05647004768252373\n",
      "  weight_norm: 11.319428443908691\n",
      "  bias_mean: -0.0010233576176688075\n",
      "  bias_std: 0.02931881882250309\n",
      "  bias_max: 0.0511067658662796\n",
      "  bias_min: -0.05225969851016998\n",
      "  bias_norm: 0.5741308927536011\n",
      "\n",
      "Layer: blocks.5.ffwd.net.0\n",
      "  weight_mean: -6.293519163591554e-06\n",
      "  weight_std: 0.02954430878162384\n",
      "  weight_max: 0.06354019045829773\n",
      "  weight_min: -0.06553839892148972\n",
      "  weight_norm: 22.69001007080078\n",
      "  bias_mean: -1.8832477508112788e-05\n",
      "  bias_std: 0.029350366443395615\n",
      "  bias_max: 0.051936063915491104\n",
      "  bias_min: -0.05370643362402916\n",
      "  bias_norm: 1.1499205827713013\n",
      "\n",
      "Layer: blocks.5.ffwd.net.2\n",
      "  weight_mean: -2.4175946236937307e-05\n",
      "  weight_std: 0.014808254316449165\n",
      "  weight_max: 0.04304073005914688\n",
      "  weight_min: -0.035540081560611725\n",
      "  weight_norm: 11.3727445602417\n",
      "  bias_mean: 0.0005916461814194918\n",
      "  bias_std: 0.014704413712024689\n",
      "  bias_max: 0.025797659531235695\n",
      "  bias_min: -0.02632799744606018\n",
      "  bias_norm: 0.2880045175552368\n",
      "\n",
      "Layer: blocks.5.ln1\n",
      "  weight_mean: 0.9986116886138916\n",
      "  weight_std: 0.0017536832019686699\n",
      "  weight_max: 1.0054502487182617\n",
      "  weight_min: 0.994125247001648\n",
      "  weight_norm: 19.568742752075195\n",
      "  bias_mean: 0.00016380618035327643\n",
      "  bias_std: 0.001301971497014165\n",
      "  bias_max: 0.005884735845029354\n",
      "  bias_min: -0.0036659117322415113\n",
      "  bias_norm: 0.025681480765342712\n",
      "\n",
      "Layer: blocks.5.ln2\n",
      "  weight_mean: 0.998643159866333\n",
      "  weight_std: 0.0025424230843782425\n",
      "  weight_max: 1.0054579973220825\n",
      "  weight_min: 0.9917420148849487\n",
      "  weight_norm: 19.569393157958984\n",
      "  bias_mean: 8.708439418114722e-05\n",
      "  bias_std: 0.0016163071850314736\n",
      "  bias_max: 0.00577282439917326\n",
      "  bias_min: -0.00511609110981226\n",
      "  bias_norm: 0.03167775273323059\n",
      "\n",
      "Layer: ln_f\n",
      "  weight_mean: 1.0\n",
      "  weight_std: 0.0\n",
      "  weight_max: 1.0\n",
      "  weight_min: 1.0\n",
      "  weight_norm: 19.595918655395508\n",
      "  bias_mean: 0.0\n",
      "  bias_std: 0.0\n",
      "  bias_max: 0.0\n",
      "  bias_min: 0.0\n",
      "  bias_norm: 0.0\n",
      "\n",
      "Layer: lm_head\n",
      "  weight_mean: 0.0001200820624944754\n",
      "  weight_std: 0.02893390692770481\n",
      "  weight_max: 0.06326363235712051\n",
      "  weight_min: -0.06520284712314606\n",
      "  weight_norm: 4.571138858795166\n",
      "  bias_mean: 0.0015633248258382082\n",
      "  bias_std: 0.02706340327858925\n",
      "  bias_max: 0.05018534138798714\n",
      "  bias_min: -0.05250523239374161\n",
      "  bias_norm: 0.21687379479408264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def layer_statistics(model):\n",
    "    stats = {}\n",
    "    for name, layer in model.named_modules():\n",
    "        # Check if the layer has weights (e.g., Linear, Conv layers)\n",
    "        if hasattr(layer, 'weight') and hasattr(layer.weight, 'data'):\n",
    "            stats[name] = {\n",
    "                'weight_mean': layer.weight.data.mean().item(),\n",
    "                'weight_std': layer.weight.data.std().item(),\n",
    "                'weight_max': layer.weight.data.max().item(),\n",
    "                'weight_min': layer.weight.data.min().item(),\n",
    "                'weight_norm': layer.weight.data.norm().item(),\n",
    "            }\n",
    "            # Check if the layer has biases\n",
    "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
    "                stats[name].update({\n",
    "                    'bias_mean': layer.bias.data.mean().item(),\n",
    "                    'bias_std': layer.bias.data.std().item(),\n",
    "                    'bias_max': layer.bias.data.max().item(),\n",
    "                    'bias_min': layer.bias.data.min().item(),\n",
    "                    'bias_norm': layer.bias.data.norm().item(),\n",
    "                })\n",
    "    return stats\n",
    "\n",
    "# Assuming 'model' is your BigramLanguageModel instance\n",
    "stats = layer_statistics(model)\n",
    "\n",
    "# Print out the statistics\n",
    "for layer_name, layer_stats in stats.items():\n",
    "    print(f\"Layer: {layer_name}\")\n",
    "    for stat_name, stat_value in layer_stats.items():\n",
    "        print(f\"  {stat_name}: {stat_value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'model' is your BigramLanguageModel instance\n",
    "# and 'input_data' is a tensor representing your input\n",
    "\n",
    "# Step 2: Define a hook to capture the activations\n",
    "activations = []\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations.append(output.detach())\n",
    "    return hook\n",
    "\n",
    "# Register the hook to the ReLU activations in the feedforward blocks\n",
    "for name, block in model.blocks.named_children():\n",
    "    block.ffwd.net[1].register_forward_hook(get_activation(name))\n",
    "\n",
    "# Step 3: Run the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    _ = model(input_data)\n",
    "\n",
    "# Step 4: Plot the activations\n",
    "# You might have multiple sets of activations, one for each block\n",
    "# Here we plot the activations for the first block as an example\n",
    "plt.figure(figsize=(20, 10))\n",
    "# Assuming the activations are stored in a list and we're interested in the first one\n",
    "# You may need to adjust the index based on which layer's activations you want to plot\n",
    "plt.imshow(activations[0].abs() > 0.99, cmap='gray', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Remove hooks (important to prevent memory leaks)\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the rest of your model and functions are defined as before\n",
    "\n",
    "# Register the hook to the ReLU activations in the feedforward blocks\n",
    "activations = []\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations.append(output.detach())\n",
    "    return hook\n",
    "\n",
    "# Choose a specific block and ReLU layer to attach the hook\n",
    "# Adjust the index based on which block's activations you want to plot\n",
    "model.blocks[0].ffwd.net[1].register_forward_hook(get_activation('block0_relu'))\n",
    "\n",
    "# Get a batch of validation data\n",
    "X, _ = get_batch('val')\n",
    "\n",
    "# Run the model to capture the activations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(X)\n",
    "\n",
    "# # Plot the activations\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# # Assuming the activations are stored in a list and we're interested in the first one\n",
    "# plt.imshow(activations[0].abs().cpu() > 0.99, cmap='gray', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "# Select the first sequence in the batch and the first feature across all positions\n",
    "activations_2d = activations[0][:, :, 0].cpu()\n",
    "\n",
    "# Now plot the activations for this single feature\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(activations_2d.abs() > 0.99, cmap='gray', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Remove the hook after use to prevent memory leaks\n",
    "model.blocks[0].ffwd.net[1]._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the rest of your model and functions are defined as before\n",
    "\n",
    "# Register the hook to the ReLU activations in the feedforward blocks\n",
    "activations = []\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations.append(output.detach())\n",
    "    return hook\n",
    "\n",
    "# Choose a specific block and ReLU layer to attach the hook\n",
    "# Adjust the index based on which block's activations you want to plot\n",
    "model.blocks[0].ffwd.net[1].register_forward_hook(get_activation('block0_relu'))\n",
    "\n",
    "# Get a batch of validation data\n",
    "X, _ = get_batch('val')\n",
    "\n",
    "# Run the model to capture the activations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(X)\n",
    "\n",
    "# Average over the feature dimension\n",
    "activations_2d = activations[0].mean(dim=2).cpu()\n",
    "\n",
    "# Now plot the averaged activations\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(activations_2d.abs() > 0.99, cmap='gray', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "# Remove the hook after use to prevent memory leaks\n",
    "model.blocks[0].ffwd.net[1]._forward_hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Thirdings:\n",
      "Why torneys o'er the queen and ill consent,\n",
      "The heavens lies sell, and benefit doth die;\n",
      "There to kill'd the haired and the awhile.\n",
      "Fear you unwillingness in raged to anoventeen.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Strike it like life was the sea:\n",
      "Why, he hath that, were's at all the flocket.\n",
      "\n",
      "GLOUCESTER:\n",
      "Be coverfully the accopplaint.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Hark, you reprote in the east; and, as yourself\n",
      "Your husband myself constance; to see the day it be depositife:\n",
      "Blaw the people condurAge now,\n",
      "And would: I have tell my dead with statings;\n",
      "Like so my no fair day I play them by spegght,\n",
      "I rave past ill to it tear these arise.\n",
      "First, if thou dwell not be a cold tale:\n",
      "Hie were it in at liar\n",
      "Too frozen renownest quade or staip'd moars;\n",
      "Verenting a master territy I should nave,\n",
      "Plebers more lie eneninger truth.\n",
      "Thus combs the wrong, before such and over you\n",
      "Leong bey enow'd the deputy bark of policion.\n",
      "\n",
      "RIVERS:\n",
      "Dates, my lord;\n",
      "Or six fains begging them like a mazzre and\n",
      "And urge I give by words, lose; never let the surfeit,\n",
      "Who, take nomissip so to gued men tilled,\n",
      "As every sad here are.\n",
      "\n",
      "Lord RICHARD:\n",
      "Thy king is for a sair, oping chare statate.\n",
      "Farewell, reasing thy fear, than such e malls,\n",
      "And laughter youth of his face.\n",
      "No bottage up a free winjusiever slarewards,\n",
      "For no rivilege afress require should be set;\n",
      "And, as that, which shall befare me.\n",
      "Farwixt thou be the foot, I say! you must, or that I\n",
      "Tran Quie to come 'lady to acconclus;\n",
      "And do me your king in Apollo's mine,\n",
      "When I should refuge till wrong the treason,\n",
      "The rainual who comes! Killain souls to this blaze,\n",
      "Who did sent me through old knowledge\n",
      "T fother, the guilty of the brother wits,\n",
      "Have well dukes on minecalarur bench'd the deep,\n",
      "With his pilgrent from the sway of jdy ox$xe in us.\n",
      "But thou be those souls not now, brother prophecious\n",
      "Having we have donestroy'd, make affordance\n",
      "With the rests are greatness for confidence:\n",
      "Go, hence, some were straight or cook, having wounded,\n",
      "Give a wofan hour hath the forward in that's woe\n",
      "Which so many winter field; for thy will be\n",
      "falser present that those would have at once\n",
      "more folly with hever with providegr'd tires:\n",
      "He is not in hisin in one:\n",
      "He sight his for title gods it good\n",
      "For her norrow, that would you dear me?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Then, I had no less to my belded talk order king.\n",
      "Shall we wept at my son far of your meant.\n",
      "Have prick'd in justice out them in the wand\n",
      "From thy life: torrow thou deptrem'd me in his blood.\n",
      "If thou foad with a full villain, I\n",
      "Were best I cut, sir, thou\n",
      "I had one one heard that I can deliver meman,\n",
      "Command me not come to get thee with thy dreadful time:\n",
      "Then he's shorts mouthing by thee lrong are\n",
      "To bastand Bolingbroke: and I, marry thou stand\n",
      "With all from put lights, that thou do know\n",
      "Bear him not because at lives.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "My lord, but on sleepingly servy son\n",
      "Which shall be contract pliked by the poolant,\n",
      "My watchful world's gagestratish liking in dayligence.\n",
      "Edward prison, who, soeth this yong lie tou woe.\n",
      "\n",
      "KING RICHARD III:\n",
      "How nurse, I by the madam of high.\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "Kindes Perk his Siciliar, So too!\n",
      "Unprecies the King Richard to be one mean,\n",
      "Do I'll resistingly hollow ourselves to a groand;\n",
      "Yet, at he's Sight's malighty absens; thou,\n",
      "A governners can no steward slain,--out--as flattererused proper;\n",
      "Will exerch thee rog,--for thyself and hearing by.\n",
      "\n",
      "LADY RICHARD:\n",
      "I will part the innocencit with I partly vows\n",
      "Or ceptration start take had fro Come Montague,\n",
      "A whind two murderner to thy head yoke to woo.\n",
      "Ah, as for my nobler thing!\n",
      "Come, my man temper bring it well,\n",
      "Or night downrightful, I leap her speedly to;\n",
      "Oh means to what hath he would go how how to his king;\n",
      "For honour, nor any that doth this royal cource,\n",
      "Of him, with whiles ere so deadly until to men,\n",
      "And I lall for maintain'd a pale obach to bed\n",
      "I have grant. Which we banish'd effect his tedious gold,\n",
      "And frown for the cast, was seen foebling could,\n",
      "And nobher'd. Well, my friends shall stay.\n",
      "O, drew next, and strength now, of my society:\n",
      "Rest my Caitesby joint, no look upon good\n",
      "Can wot. Come Lart's friend acompt thee!\n",
      "Have mirrim that shelt been of our own tongue\n",
      "Lives in Rosafen proforce? Is he fell so parce\n",
      "Within Sofficerse?\n",
      "\n",
      "YORK:\n",
      "Fair, who shall I love them the purpose.\n",
      "\n",
      "NARGARDINE:\n",
      "Well, let them battle:\n",
      "Sit still their state King canswer Esely deforge.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well him plant?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "Thy scattaryings are towards. Why look is such fi\n",
      "Thy brows of us king?\n",
      "Thy trenches to hawTHere deplent moves?\n",
      "Ees sound too losse, for trive lacks as father:\n",
      "Here with thus far forth her feariness,\n",
      "Bless this liking cobject, were suck a tear;\n",
      "But is dam to the wearing of hope in Frence,\n",
      "Haplest tattell our court? what then?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Here's Richard, you unlisp your lord's toming,\n",
      "Harpens the wronger tinders and at Richard's rose.\n",
      "How now! whiles rudumpets O, I'll know not upon the sun patience?\n",
      "\n",
      "MARCIUS:\n",
      "Yes, father, and, if my bless, from your companion?\n",
      "\n",
      "BARCKINGHAM:\n",
      "My lord, and doth apprehend h\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the model state\n",
    "checkpoint_path = 'model_checkpoint.pth'\n",
    "\n",
    "# Create a dictionary to save all necessary information\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'hyperparameters': {\n",
    "        'batch_size': batch_size,\n",
    "        'block_size': context_length,\n",
    "        'max_iters': max_iters,\n",
    "        'eval_interval': eval_interval,\n",
    "        'learning_rate': learning_rate,\n",
    "        'device': device,\n",
    "        'eval_iters': eval_iters,\n",
    "        'n_embd': number_embeddings,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layer': n_layer,\n",
    "        'dropout': dropout,\n",
    "        'vocab_size': vocab_size\n",
    "    },\n",
    "    'additional_info': {\n",
    "        # Add any other information you want to save, such as training history, current iteration, etc.\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the checkpoint\n",
    "torch.save(checkpoint, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the file path where you want to save the model state\n",
    "checkpoint_path = 'backup/model_checkpoint.pth'\n",
    "# Load the saved checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Restore the model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Ensure the model and optimizer are in the correct mode (training or evaluation)\n",
    "model.train()  # or model.eval() depending on the mode you saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 43, 50, 50, 53]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([20, 43, 50, 50, 53], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.tensor(encode(\"Hello\"), dtype=torch.long, device=device)\n",
    "print(prompt.size())\n",
    "print(prompt.stride())\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "(5, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[20, 43, 50, 50, 53]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = prompt.unsqueeze(0)\n",
    "print(prompt.size())\n",
    "print(prompt.stride())\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hellows birds to see his hate with disposition.\n",
      "\n",
      "RICHMOND:\n",
      "I am long in this morning.\n",
      "Who doth the write \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(prompt, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "(16, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[23, 21, 26, 19,  1, 15, 20, 13, 30, 24, 17, 31,  1, 21, 21, 21]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.tensor(encode(\"KING CHARLES III\"), dtype=torch.long, device=device)\n",
    "prompt = prompt.unsqueeze(0)\n",
    "print(prompt.size())\n",
    "print(prompt.stride())\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KING CHARLES III:\n",
      "How the prison?\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Call thee unto me, my Lord of Warwick of Welcome.\n",
      "\n",
      "QUEEN:\n",
      "Why, are you not so? what stay?\n",
      "\n",
      "KING RICHARD III:\n",
      "Be thy inward of thee?\n",
      "\n",
      "Second Keeper:\n",
      "Here's no so.\n",
      "Ah,\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(prompt, max_new_tokens=200)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp-crlRM3oL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
